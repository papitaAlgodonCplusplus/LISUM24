{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psk6b0nPUpD0"
      },
      "source": [
        "# Problem Statement\n",
        "\n",
        "The large company who is into beverages business in Australia. They sell their products through various super-markets and also engage into heavy promotions throughout the year. Their demand is also influenced by various factors like holiday, seasonality. They needed forecast of each of products at item level every week in weekly buckets.\n",
        "\n",
        "# Objective\n",
        "\n",
        "1. Build at least 4-5 multivariable forecasting model which included ML or Deep Learning based Model in PySpark leveraging parallel computing techniques.\n",
        "\n",
        "2. Demonstrate best in class forecast accuracy (Forecast Accuracy = 1 - Wt. MAPE where Wt. MAPE = sum(Error)/sum(Actual).\n",
        "\n",
        "3. Write a code in such a way you run the model in least time.\n",
        "\n",
        "4. Demonstrate explainability in the form of contribution of each variables\n",
        "\n",
        "5. Leveage Feature Engineering concepts to derive more variables to gain accuracy improvement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H151dD5OVuvH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/papitaAlgodonCplusplus/LISUM24/main/Week%207/Datatset/forecasting_case_study.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X65XiNnZWFeH"
      },
      "source": [
        "# Dataset Raw Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga1c7Jm5WElw",
        "outputId": "51d60c5c-6eeb-43c8-d78d-6d4bc896a9c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Product        date   Sales Price Discount (%)  In-Store Promo  \\\n",
              "0       SKU1    2/5/2017   27750                 0%               0   \n",
              "1       SKU1   2/12/2017   29023                 0%               1   \n",
              "2       SKU1   2/19/2017   45630                17%               0   \n",
              "3       SKU1   2/26/2017   26789                 0%               1   \n",
              "4       SKU1    3/5/2017   41999                17%               0   \n",
              "...      ...         ...     ...                ...             ...   \n",
              "1213    SKU6  10/18/2020   96619                54%               0   \n",
              "1214    SKU6  10/25/2020  115798                52%               0   \n",
              "1215    SKU6   11/1/2020  152186                54%               1   \n",
              "1216    SKU6   11/8/2020   26445                44%               1   \n",
              "1217    SKU6  11/15/2020   26414                44%               0   \n",
              "\n",
              "      Catalogue Promo  Store End Promo  Google_Mobility  Covid_Flag  V_DAY  \\\n",
              "0                   0                0             0.00           0      0   \n",
              "1                   0                1             0.00           0      1   \n",
              "2                   0                0             0.00           0      0   \n",
              "3                   0                1             0.00           0      0   \n",
              "4                   0                0             0.00           0      0   \n",
              "...               ...              ...              ...         ...    ...   \n",
              "1213                1                0            -7.56           1      0   \n",
              "1214                1                0            -8.39           1      0   \n",
              "1215                0                1            -7.43           1      0   \n",
              "1216                0                1            -5.95           1      0   \n",
              "1217                0                0            -7.20           1      0   \n",
              "\n",
              "      EASTER  CHRISTMAS  \n",
              "0          0          0  \n",
              "1          0          0  \n",
              "2          0          0  \n",
              "3          0          0  \n",
              "4          0          0  \n",
              "...      ...        ...  \n",
              "1213       0          0  \n",
              "1214       0          0  \n",
              "1215       0          0  \n",
              "1216       0          0  \n",
              "1217       0          0  \n",
              "\n",
              "[1218 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-03b4270c-5562-4b41-9eb2-a18b52f25960\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product</th>\n",
              "      <th>date</th>\n",
              "      <th>Sales</th>\n",
              "      <th>Price Discount (%)</th>\n",
              "      <th>In-Store Promo</th>\n",
              "      <th>Catalogue Promo</th>\n",
              "      <th>Store End Promo</th>\n",
              "      <th>Google_Mobility</th>\n",
              "      <th>Covid_Flag</th>\n",
              "      <th>V_DAY</th>\n",
              "      <th>EASTER</th>\n",
              "      <th>CHRISTMAS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SKU1</td>\n",
              "      <td>2/5/2017</td>\n",
              "      <td>27750</td>\n",
              "      <td>0%</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SKU1</td>\n",
              "      <td>2/12/2017</td>\n",
              "      <td>29023</td>\n",
              "      <td>0%</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SKU1</td>\n",
              "      <td>2/19/2017</td>\n",
              "      <td>45630</td>\n",
              "      <td>17%</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SKU1</td>\n",
              "      <td>2/26/2017</td>\n",
              "      <td>26789</td>\n",
              "      <td>0%</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SKU1</td>\n",
              "      <td>3/5/2017</td>\n",
              "      <td>41999</td>\n",
              "      <td>17%</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1213</th>\n",
              "      <td>SKU6</td>\n",
              "      <td>10/18/2020</td>\n",
              "      <td>96619</td>\n",
              "      <td>54%</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-7.56</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1214</th>\n",
              "      <td>SKU6</td>\n",
              "      <td>10/25/2020</td>\n",
              "      <td>115798</td>\n",
              "      <td>52%</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-8.39</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1215</th>\n",
              "      <td>SKU6</td>\n",
              "      <td>11/1/2020</td>\n",
              "      <td>152186</td>\n",
              "      <td>54%</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-7.43</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1216</th>\n",
              "      <td>SKU6</td>\n",
              "      <td>11/8/2020</td>\n",
              "      <td>26445</td>\n",
              "      <td>44%</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-5.95</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1217</th>\n",
              "      <td>SKU6</td>\n",
              "      <td>11/15/2020</td>\n",
              "      <td>26414</td>\n",
              "      <td>44%</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-7.20</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1218 rows Ã— 12 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03b4270c-5562-4b41-9eb2-a18b52f25960')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-03b4270c-5562-4b41-9eb2-a18b52f25960 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-03b4270c-5562-4b41-9eb2-a18b52f25960');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8f391b49-8115-4d37-a2f7-b1286c12c12c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8f391b49-8115-4d37-a2f7-b1286c12c12c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8f391b49-8115-4d37-a2f7-b1286c12c12c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmJ2yAdQk4x2"
      },
      "source": [
        "# Features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeuFvSJPd-BV"
      },
      "source": [
        "* **Product:** X Product that the company offers to their clients\n",
        "\n",
        "* **Date:** *Month - Day - Year* where row X transactions ocurred\n",
        "\n",
        "* **Sales:** Number of products X sold that day\n",
        "\n",
        "* **Price Discount (%):** Discount applied to the product X that day\n",
        "\n",
        "* **In-Store Promo:** Wheter there was promotion of that product indoors or not\n",
        "\n",
        "* **Catalogue Promo:** Wheter there was promotion of that product in the catalogue or not\n",
        "\n",
        "* **Store End Promo:** Wheter there was promotion of that product outdoors or not\n",
        "\n",
        "* **Google_Mobility:** Google Mobility, also known as Google COVID-19 Community Mobility Reports, is a data initiative launched by Google to provide insights into how people's movements and activities have changed in response to the COVID-19 pandemic, this as a feature column with ranges [-inf, inf] means wheter X product sales during Y date were supossed to change or not, and by how much.\n",
        "\n",
        "* **Covid_Flag:** A binary column that only activates when google mobility predicts changes on sales of X product during Y date.\n",
        "\n",
        "* **V_DAY:** Wheter that day was Valentine's day for the company or not\n",
        "\n",
        "* **Easter:** Wheter that day was Easter for the company or not\n",
        "\n",
        "* **Christmas:** Wheter that day was Christmas for the company or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEsunGu2kScm"
      },
      "source": [
        "# Data types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AyMJmlwjYDU"
      },
      "source": [
        "**Categorical Data:**\n",
        "- Product: X Product that the company offers to their clients\n",
        "\n",
        "**Continuous Numerical Data:**\n",
        "- Date: Month - Day - Year where row X transactions occurred (If changed to another analogous format)\n",
        "- Sales: Number of products X sold that day\n",
        "- Price Discount (%): Discount applied to product X that day\n",
        "- Google_Mobility: Google Mobility index indicating the change in sales due to the COVID-19 pandemic (ranges [-inf, inf])\n",
        "\n",
        "**Binary Data:**\n",
        "- In-Store Promo: Whether there was an indoor promotion of that product or not\n",
        "- Catalogue Promo: Whether there was a promotion of that product in the catalogue or not\n",
        "- Store End Promo: Whether there was an outdoor promotion of that product or not\n",
        "- Covid_Flag: A binary column that activates when Google Mobility predicts changes in sales of X product during Y date\n",
        "- V_DAY: Whether that day was Valentine's day for the company or not\n",
        "- Easter: Whether that day was Easter for the company or not\n",
        "- Christmas: Whether that day was Christmas for the company or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OImpKBK6k8mS"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFRF_X4hlBgU"
      },
      "source": [
        "## NA values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtIMkrAdmCj1"
      },
      "source": [
        "<font color= 'purple'>**Approach:**</font> First let's see if there are any NA values to handle, if so, either mean or mode could be applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltSMkuyceBYl",
        "outputId": "e50cf9d6-18c6-4a77-dde0-73f763bfa050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Product: 0\n",
            "date: 0\n",
            "Sales: 0\n",
            "Price Discount (%): 0\n",
            "In-Store Promo: 0\n",
            "Catalogue Promo: 0\n",
            "Store End Promo: 0\n",
            "Google_Mobility: 0\n",
            "Covid_Flag: 0\n",
            "V_DAY: 0\n",
            "EASTER: 0\n",
            "CHRISTMAS: 0\n"
          ]
        }
      ],
      "source": [
        "na_counts = df.isna().sum()\n",
        "\n",
        "for column, na_count in na_counts.items():\n",
        "    print(f\"{column}: {na_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSLRWJ3gmFy4"
      },
      "source": [
        "## Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tSR1prZfmHcG"
      },
      "outputs": [],
      "source": [
        "def plot_data(df, column, sort=False, x=None):\n",
        "  if sort:\n",
        "    sorted_df = df.sort_values(by=column)\n",
        "    plt.plot(sorted_df[column], sorted_df[x])\n",
        "    plt.title('Sorted: '+ column + ' by ' + x)\n",
        "    plt.xlabel(x)\n",
        "    plt.ylabel(column)\n",
        "    plt.show()\n",
        "\n",
        "  else:\n",
        "    # Plotting numerical column\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if x != None:\n",
        "      plt.scatter(df[column], df[x])\n",
        "      plt.xlabel(column)\n",
        "      plt.ylabel(x)\n",
        "      plt.title('Scatter Plot')\n",
        "      plt.show()\n",
        "    else:\n",
        "      plt.plot(df[column])\n",
        "      plt.title(column)\n",
        "      plt.show()\n",
        "\n",
        "def delete_noise(df, column, reduction_method='IQR', error_margin=1.5, plot_noise=True, threshold=4, renew=True):\n",
        "  if reduction_method == 'IQR':\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - error_margin * IQR\n",
        "    upper_bound = Q3 + error_margin * IQR\n",
        "\n",
        "    noisy_indices = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
        "\n",
        "    if plot_noise:\n",
        "      df['Is_Noisy'] = noisy_indices\n",
        "\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      plt.scatter(df.index, df[column], c=df['Is_Noisy'], cmap='coolwarm', marker='o')\n",
        "      plt.title('Identified Outliers')\n",
        "      plt.colorbar(label='Outlier (1) / Non-outlier (0)')\n",
        "      plt.show()\n",
        "\n",
        "      df.drop('Is_Noisy', axis=1, inplace=True)\n",
        "\n",
        "    if renew:\n",
        "      return df.drop(df.index[noisy_indices]).copy()\n",
        "\n",
        "  if reduction_method == 'Statistical':\n",
        "    mean = df[column].mean()\n",
        "    std_dev = df[column].std()\n",
        "\n",
        "    noise_threshold = mean + threshold * std_dev\n",
        "    noisy_indices = (df[column] - mean).abs() > threshold * std_dev\n",
        "\n",
        "    if plot_noise:\n",
        "      df['Is_Noisy'] = noisy_indices\n",
        "\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      plt.scatter(df.index, df[column], c=df['Is_Noisy'], cmap='coolwarm', marker='o')\n",
        "      plt.title('Identified Outliers')\n",
        "      plt.colorbar(label='Outlier (1) / Non-outlier (0)')\n",
        "      plt.show()\n",
        "\n",
        "      df.drop('Is_Noisy', axis=1, inplace=True)\n",
        "\n",
        "    if renew:\n",
        "      return df[df[column] <= noise_threshold]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lfNseWRp4qY"
      },
      "source": [
        "### Google Mobility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Geo5CMf4BxPF"
      },
      "source": [
        "#### Stadistical Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyY9z4i2p-7P"
      },
      "outputs": [],
      "source": [
        "test_df = delete_noise(df, 'Google_Mobility', reduction_method='Statistical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_2tPAYmBzs_"
      },
      "source": [
        "#### IQR Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLGX586SB1du"
      },
      "outputs": [],
      "source": [
        "test_df = delete_noise(df, 'Google_Mobility')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIm1wmHRB5Vl"
      },
      "source": [
        "#### Am I deleting outliners?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRHOO3y-qTxB"
      },
      "source": [
        "<font color='red'>**Discarded:**</font> Because the outliers are actually a pattern and is not greater than 30% decrease prediction (which is lucid in the context of COVID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4MmF-VXs-R4"
      },
      "source": [
        "### Discount"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRyfBZEiCDT8"
      },
      "source": [
        "#### Stadistical Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aTD6vCztGMc"
      },
      "outputs": [],
      "source": [
        "df['Price Discount (%)'] = df['Price Discount (%)'].str.replace('%', '').astype(float)\n",
        "test_df = delete_noise(df, 'Price Discount (%)', reduction_method='Statistical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ajg9AU5CF3F"
      },
      "source": [
        "#### IQR Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMkwXajRCI44"
      },
      "outputs": [],
      "source": [
        "test_df = delete_noise(df, 'Price Discount (%)', error_margin=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM-mGNF8CWen"
      },
      "source": [
        "#### Am I deleting outliners?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR2WWa3ruW8b"
      },
      "source": [
        "<font color='red'>**Discarded:**</font> Because the outliers % of discount are  common in the context of this company's timeline given some season and acceptation would only generate weights misplacement later on model training and tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IujkEYr-rJJE"
      },
      "source": [
        "With this, outlier detection is concluded, as there is no binary noise to delete statistically and 'Sales' is the label/target, not a feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEOyMwI-3IGs"
      },
      "source": [
        "## Skewed Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV5gkwa65Yzr"
      },
      "source": [
        "### Google Mobility Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aabDhaJ75yg"
      },
      "source": [
        "<font color='green'>**Accepted:**</font> Normalization helps to ensure faster convergence and prevents biased magnitude domination during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW16rk75lo6o"
      },
      "source": [
        "<font color= 'purple'>**Approach:**</font> First step is to create a function to fastly plot the resulting values of the normalized columns, then, normalization is applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eK4dT3Xv3XBo"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot(df, *columns, title=\"\"):\n",
        "    fig = go.Figure()\n",
        "\n",
        "    for column in columns:\n",
        "        fig.add_trace(go.Scatter(y=df[column], mode='lines+markers', name=column, opacity=0.7))\n",
        "\n",
        "    fig.update_layout(\n",
        "        xaxis_title=\"Index\",\n",
        "        yaxis_title=\"Value\",\n",
        "        title=title,\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "plot(df, 'Google_Mobility')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2LKDNYw5dQp"
      },
      "outputs": [],
      "source": [
        "min = df['Google_Mobility'].min()\n",
        "max = df['Google_Mobility'].max()\n",
        "\n",
        "df['Google_Mobility'] = (df['Google_Mobility'] - min) / (max - min)\n",
        "\n",
        "plot(df, 'Google_Mobility')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5w_6eUEi7eE"
      },
      "source": [
        "### Google_Mobility logarithmic transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BkDKl7njD0j"
      },
      "outputs": [],
      "source": [
        "test_df['Google_Mobility'] = np.log1p(df['Google_Mobility'])\n",
        "plot(test_df, 'Google_Mobility')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2ZepSLTjX5J"
      },
      "source": [
        "<font color='red'>**Discarded:**</font> Logarithmic transformation has no effect on this column (neither price discount) because the trend stays still to the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b9CjLLm7aNs"
      },
      "source": [
        "### Price Discount (%) Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkiuoNYk7eZL"
      },
      "outputs": [],
      "source": [
        "plot(df, 'Price Discount (%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xJYr3RF_HVN"
      },
      "outputs": [],
      "source": [
        "min = df['Price Discount (%)'].min()\n",
        "max = df['Price Discount (%)'].max()\n",
        "\n",
        "df['Price Discount (%)'] = (df['Price Discount (%)'] - min) / (max - min)\n",
        "\n",
        "plot(df, 'Price Discount (%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVk_gmuwlbPO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ1ohpuvAvsD"
      },
      "source": [
        "## Date Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DxgWeshAxVJ"
      },
      "source": [
        "<font color='green'>**Accepted:**</font> Encoding 'date' column is necessary so the chosen model can make forecast predictions over seasonality patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGOjcrEclNe5"
      },
      "source": [
        "<font color= 'purple'>**Approach:**</font> Using the datetime option from pandas, is easy to extract year, month and day for each row into new columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKuyWA3aBCRL"
      },
      "outputs": [],
      "source": [
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "df['year'] = df['date'].dt.year\n",
        "df['month'] = df['date'].dt.month\n",
        "df['day'] = df['date'].dt.day"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0LLuTFXksnX"
      },
      "source": [
        "## Aggregation for Time Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr53bt2Hkzu1"
      },
      "source": [
        "<font color='green'>**Accepted:**</font> Aggregating the whole dataframe to obtain 2 new datasets based on year and month could be beneficial both for EDA and Training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pznE7RwLoZ9F"
      },
      "source": [
        "<font color= 'purple'>**Approach:**</font> First step is grouping the dataframe by month or year, apply aggregation based on 4 columns selected to EDA:\n",
        "\n",
        "1. Sales by it's total sum over the month/year\n",
        "2. Discount as the standard deviation it had during the month/year\n",
        "3. Google Mobility by it's mean value during the month/year\n",
        "4. Product by creating 6 different aggregated dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFgGsRvWpA8w"
      },
      "source": [
        "### By month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fejsPLA7kzIq"
      },
      "outputs": [],
      "source": [
        "df_sku1 = df[df['Product'] == 'SKU1']\n",
        "df_sku2 = df[df['Product'] == 'SKU2']\n",
        "df_sku3 = df[df['Product'] == 'SKU3']\n",
        "df_sku4 = df[df['Product'] == 'SKU4']\n",
        "df_sku5 = df[df['Product'] == 'SKU5']\n",
        "df_sku6 = df[df['Product'] == 'SKU6']\n",
        "\n",
        "def aggregate(df, target):\n",
        "  local_df = df.groupby(df['date'].dt.to_period(target)).agg({\n",
        "      'Sales': 'sum',\n",
        "      'Price Discount (%)': 'std',\n",
        "      'Google_Mobility': 'mean',\n",
        "      'Product': 'count'\n",
        "  }).reset_index()\n",
        "  return local_df\n",
        "\n",
        "monthly_aggregated_sku1 = aggregate(df_sku1, 'M')\n",
        "monthly_aggregated_sku2 = aggregate(df_sku2, 'M')\n",
        "monthly_aggregated_sku3 = aggregate(df_sku3, 'M')\n",
        "monthly_aggregated_sku4 = aggregate(df_sku4, 'M')\n",
        "monthly_aggregated_sku5 = aggregate(df_sku5, 'M')\n",
        "monthly_aggregated_sku6 = aggregate(df_sku6, 'M')\n",
        "plot(monthly_aggregated_sku3, 'Sales')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCsz5a1KpCex"
      },
      "source": [
        "### By year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBUHGsoapGXx"
      },
      "outputs": [],
      "source": [
        "yearly_aggregated_sku1 = aggregate(df_sku1, 'Y')\n",
        "yearly_aggregated_sku2 = aggregate(df_sku2, 'Y')\n",
        "yearly_aggregated_sku3 = aggregate(df_sku3, 'Y')\n",
        "yearly_aggregated_sku4 = aggregate(df_sku4, 'Y')\n",
        "yearly_aggregated_sku5 = aggregate(df_sku5, 'Y')\n",
        "yearly_aggregated_sku6 = aggregate(df_sku6, 'Y')\n",
        "\n",
        "plot(yearly_aggregated_sku6, 'Product')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yJ3osj_kGL7"
      },
      "source": [
        "This concludes data preprocessing, now all the data is normalized and has no issues to resolve before entering training stage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOH2aYnnpGt9"
      },
      "source": [
        "# EDA & Feature Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qUCYMJKv7Qz"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEG62JUSwx1C"
      },
      "source": [
        "### Lag Period Difference (A.K.A Derivative Function)\n",
        "\n",
        "This will return a column with the difference or change ratio of the sales values based on the previous sale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orcopm0Szg6W"
      },
      "source": [
        "#### Global Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJkepBHAwzc-"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "df_sorted_date = df.sort_values(by='date')\n",
        "df_sorted_date = df_sorted_date.reset_index(drop=True)\n",
        "\n",
        "shifted_sales = df_sorted_date['Sales'].shift(1)\n",
        "df_sorted_date['Change_Ratio'] = df_sorted_date['Sales'] - shifted_sales\n",
        "\n",
        "fig = px.line(df_sorted_date, x=df_sorted_date.index, y=['Sales', 'Change_Ratio'],\n",
        "              labels={'x': 'Date', 'y': 'Sales'},\n",
        "              title='Sales and Change Ratio')\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9620aQxzkT_"
      },
      "source": [
        "#### Monthly Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXXNUZ_NzmYB"
      },
      "outputs": [],
      "source": [
        "monthly_dfs = [monthly_aggregated_sku1, monthly_aggregated_sku2, monthly_aggregated_sku3, monthly_aggregated_sku4,\n",
        "               monthly_aggregated_sku5, monthly_aggregated_sku6]\n",
        "\n",
        "for idx, dataframe in enumerate(monthly_dfs):\n",
        "    shifted_sales = dataframe['Sales'].shift(1)\n",
        "    dataframe['Change_Ratio'] = dataframe['Sales'] - shifted_sales\n",
        "    monthly_dfs[idx] = dataframe\n",
        "\n",
        "combined_df = pd.concat([dataframe['Change_Ratio'] for dataframe in monthly_dfs], axis=1)\n",
        "combined_df.columns = [f\"SKU{idx+1}\" for idx in range(len(monthly_dfs))]\n",
        "combined_df.dropna(inplace=True)\n",
        "\n",
        "fig = px.line(combined_df, x=combined_df.index, y=combined_df.columns, title='Change Ratio for SKUs')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB5s0Ukw6Z8k"
      },
      "source": [
        "#### Yearly Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtEueKgZ6c1L"
      },
      "outputs": [],
      "source": [
        "yearly_dfs = [yearly_aggregated_sku1, yearly_aggregated_sku2, yearly_aggregated_sku3,\n",
        "              yearly_aggregated_sku4, yearly_aggregated_sku5, yearly_aggregated_sku6]\n",
        "\n",
        "for idx, dataframe in enumerate(yearly_dfs):\n",
        "    shifted_sales = dataframe['Sales'].shift(1)\n",
        "    dataframe['Change_Ratio'] = dataframe['Sales'] - shifted_sales\n",
        "    yearly_dfs[idx] = dataframe\n",
        "\n",
        "combined_df = pd.concat([dataframe['Change_Ratio'] for dataframe in yearly_dfs], axis=1)\n",
        "combined_df.columns = [f\"SKU{idx+1}\" for idx in range(len(yearly_dfs))]\n",
        "combined_df.dropna(inplace=True)\n",
        "\n",
        "fig = px.line(combined_df, x=combined_df.index, y=combined_df.columns, title='Change Ratio for SKUs (Yearly)')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK2m9lZaTkcK"
      },
      "source": [
        "### Rolling Average\n",
        "\n",
        "This will return a mathematical insight of the sales trend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7_fr55ETrXt"
      },
      "source": [
        "#### Sales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mrEnF-rXSeW"
      },
      "source": [
        "###### Weekly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NZQ5iYDTugD"
      },
      "outputs": [],
      "source": [
        "df_sorted_date['Rolling_Avg'] = df_sorted_date['Sales'].rolling(window=7).mean()\n",
        "plot(df_sorted_date, 'Rolling_Avg', 'Sales')\n",
        "df_sorted_date.drop('Rolling_Avg', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psAHTF3wfqQQ"
      },
      "outputs": [],
      "source": [
        "df_sorted_date['Rolling_Avg'] = df_sorted_date['Sales'].rolling(window=7).mean()\n",
        "\n",
        "min = df_sorted_date['Rolling_Avg'].min()\n",
        "max = df_sorted_date['Rolling_Avg'].max()\n",
        "\n",
        "df_sorted_date['Rolling_Avg'] = (df_sorted_date['Rolling_Avg'] - min) / (max - min)\n",
        "\n",
        "min = df_sorted_date['Sales'].min()\n",
        "max = df_sorted_date['Sales'].max()\n",
        "\n",
        "df_sorted_date['Sales_Norm'] = (df_sorted_date['Sales'] - min) / (max - min)\n",
        "\n",
        "plot(df_sorted_date, 'Rolling_Avg', 'Sales_Norm', 'Google_Mobility')\n",
        "df_sorted_date.drop('Rolling_Avg', axis=1, inplace=True)\n",
        "df_sorted_date.drop('Sales_Norm', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfW2y79Whysd"
      },
      "source": [
        "We can see Google Mobility has not any important impact over sales raw data nor tendency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ4Zlpdwcmu_"
      },
      "source": [
        "##### Log Weekly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxJsu4hGcyoI"
      },
      "outputs": [],
      "source": [
        "df_sorted_date['Rolling_Avg'] = df_sorted_date['Sales'].rolling(window=7).mean()\n",
        "df_sorted_date['Rolling_Avg'] = np.log1p(df_sorted_date['Rolling_Avg'])\n",
        "df_sorted_date['Sales_Log'] = np.log1p(df_sorted_date['Sales'])\n",
        "plot(df_sorted_date, 'Rolling_Avg', 'Sales_Log')\n",
        "df_sorted_date.drop('Rolling_Avg', axis=1, inplace=True)\n",
        "df_sorted_date.drop('Sales_Log', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPPzrYufZ4Y8"
      },
      "source": [
        "##### Monthly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiOXDM53Z2UU"
      },
      "outputs": [],
      "source": [
        "df_sorted_date['Rolling_Avg'] = df_sorted_date['Sales'].rolling(window=30).mean()\n",
        "plot(df_sorted_date, 'Rolling_Avg', 'Sales')\n",
        "df_sorted_date.drop('Rolling_Avg', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Amog_EVWQOWz"
      },
      "outputs": [],
      "source": [
        "df_sorted_date['Rolling_Avg'] = df_sorted_date['Sales'].rolling(window=30).mean()\n",
        "\n",
        "min = df_sorted_date['Rolling_Avg'].min()\n",
        "max = df_sorted_date['Rolling_Avg'].max()\n",
        "\n",
        "df_sorted_date['Rolling_Avg'] = (df_sorted_date['Rolling_Avg'] - min) / (max - min)\n",
        "\n",
        "min = df_sorted_date['Sales'].min()\n",
        "max = df_sorted_date['Sales'].max()\n",
        "\n",
        "df_sorted_date['Sales_Norm'] = (df_sorted_date['Sales'] - min) / (max - min)\n",
        "df_sorted_date['Discount_Mean'] = df_sorted_date['Price Discount (%)'].rolling(window=15).mean()\n",
        "\n",
        "plot(df_sorted_date, 'Rolling_Avg', 'Sales_Norm', 'Discount_Mean')\n",
        "df_sorted_date.drop('Rolling_Avg', axis=1, inplace=True)\n",
        "df_sorted_date.drop('Sales_Norm', axis=1, inplace=True)\n",
        "df_sorted_date.drop('Discount_Mean', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8KLuWj-RYqT"
      },
      "source": [
        "There is actually some mild patterns between the monthly rolling average of sales and biweekly discount."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kcxuuISdabx"
      },
      "source": [
        "##### Log Monthly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsInjQxPdbyU"
      },
      "outputs": [],
      "source": [
        "df_sorted_date['Rolling_Avg'] = df_sorted_date['Sales'].rolling(window=30).mean()\n",
        "df_sorted_date['Rolling_Avg'] = np.log1p(df_sorted_date['Rolling_Avg'])\n",
        "df_sorted_date['Sales_Log'] = np.log1p(df_sorted_date['Sales'])\n",
        "plot(df_sorted_date, 'Rolling_Avg', 'Sales_Log')\n",
        "df_sorted_date.drop('Rolling_Avg', axis=1, inplace=True)\n",
        "df_sorted_date.drop('Sales_Log', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztfxbygJS9Vi"
      },
      "source": [
        "#### In-Store Promotion Lag Effects\n",
        "\n",
        "This will calculate and plot the amount of \"lag\" or \"impact\" 1 day of in-store promotion has over sales, e.g. promotion during 14 February has sales increase during 14, 15, 16 and 17 of February."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9Jw6lLkVpdX"
      },
      "source": [
        "##### SKU 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoQVp1sIUnne"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "df_sku1_total = df[df['Product'] == 'SKU1']\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku1_total['date'], y=df_sku1_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku1_total.iterrows():\n",
        "    if row['In-Store Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku1_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goESaTtYXQ9Z"
      },
      "source": [
        "There is actually a strong correlation between 'yesterday' promotion and 'today's' sales, almost making it impossible to not have a promotion the day before sales where up to >= 40K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjbB6kW8X3Y3"
      },
      "source": [
        "##### SKU 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu3FIxidX3Y_"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "df_sku2_total = df[df['Product'] == 'SKU2']\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku2_total['date'], y=df_sku2_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku2_total.iterrows():\n",
        "    if row['In-Store Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku2_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjLJNLZDX3ZA"
      },
      "source": [
        "There is actually a strong correlation between 'today's' promotion and 'today's' sales, almost making it impossible to not have a promotion the day before sales where up to >= 30K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Gcbm4jYCrh"
      },
      "source": [
        "##### SKU 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v1O1J38YCr0"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "df_sku3_total = df[df['Product'] == 'SKU3']\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku3_total['date'], y=df_sku3_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku3_total.iterrows():\n",
        "    if row['In-Store Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku3_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsOQ6-0DYCr0"
      },
      "source": [
        "There is actually a strong correlation between 'yesterday' promotion and 'today's' sales, almost making it impossible to not have a promotion the day before sales where up to >= 50K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IolmeSvYSN-"
      },
      "source": [
        "##### SKU 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pma5_iOEYSOG"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "df_sku4_total = df[df['Product'] == 'SKU4']\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku4_total['date'], y=df_sku4_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku4_total.iterrows():\n",
        "    if row['In-Store Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku4_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQAtYsh5YSOG"
      },
      "source": [
        "There is actually a strong correlation between 'today's' promotion and 'today's' sales, almost making it impossible to not have a promotion the day before sales where up to >= 10K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tir6KEBjYp2V"
      },
      "source": [
        "##### SKU 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2Zssn56Yp2d"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "df_sku5_total = df[df['Product'] == 'SKU5']\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku5_total['date'], y=df_sku5_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku5_total.iterrows():\n",
        "    if row['In-Store Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku5_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McXv9ulNYp2e"
      },
      "source": [
        "There is actually a strong correlation between 'today's' promotion and 'today's' sales, almost making it impossible to not have a promotion the day before sales where up to >= 20K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwGRlUDQau77"
      },
      "source": [
        "##### SKU 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yl-vBQ0Yau8E"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "df_sku6_total = df[df['Product'] == 'SKU6']\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku6_total['date'], y=df_sku6_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku6_total.iterrows():\n",
        "    if row['In-Store Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku6_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcV_vtTBau8F"
      },
      "source": [
        "There is not a pattern between promotion and sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrmxJTo7Exxe"
      },
      "source": [
        "#### Catalogue Lag Effects\n",
        "\n",
        "This will calculate and plot the amount of \"lag\" or \"impact\" 1 day of catalogue promotion has over sales, e.g. promotion during 14 February has sales increase during 14, 15, 16 and 17 of February."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TN0pdS0Exxf"
      },
      "source": [
        "##### SKU 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HB1OsnlVExxf"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku1_total['date'], y=df_sku1_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku1_total.iterrows():\n",
        "    if row['Catalogue Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku1_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eihK9Z4TExxg"
      },
      "source": [
        "There is actually a strong correlation between '4-5 days' promotion and 'today's' sales, almost making it impossible to not have a promotion the day before sales where up to >= 40K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrTnrRPGExxg"
      },
      "source": [
        "##### SKU 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jum0VrUnExxg"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku2_total['date'], y=df_sku2_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku2_total.iterrows():\n",
        "    if row['Catalogue Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku2_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9hbYLwWExxh"
      },
      "source": [
        "There is actually a strong correlation between 'yesterday' promotion and 'today's' sales, almost making it impossible to not have a promotion the day before sales where up to >= 30K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FHkFuzCExxi"
      },
      "source": [
        "##### SKU 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_AD3hysExxi"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku3_total['date'], y=df_sku3_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku3_total.iterrows():\n",
        "    if row['Catalogue Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku3_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKZ3OC6eExxj"
      },
      "source": [
        "There is actually a strong correlation between 'yesterday' promotion and 'today's' sales, almost making it impossible to not have a promotion the day before sales where up to >= 50K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ternBks8Exxj"
      },
      "source": [
        "##### SKU 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCqsD8ayExxj"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku4_total['date'], y=df_sku4_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku4_total.iterrows():\n",
        "    if row['Catalogue Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku4_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLi-cmc0Exxk"
      },
      "source": [
        "There is actually a strong correlation between 'yesterday' promotion and 'today's' sales, almost making it impossible to not have a promotion the day before sales where up to >= 10K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "582AknIpExxk"
      },
      "source": [
        "##### SKU 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUivCDs3Exxk"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku5_total['date'], y=df_sku5_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku5_total.iterrows():\n",
        "    if row['Catalogue Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku5_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDsjj2kpExxl"
      },
      "source": [
        "There is actually a strong correlation between 'yesterday' promotion and 'today's' sales, almost making it impossible to not have a promotion the day before sales where up to >= 20K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zghs7wwxExxl"
      },
      "source": [
        "##### SKU 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBy9TJkJExxl"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku6_total['date'], y=df_sku6_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku6_total.iterrows():\n",
        "    if row['Catalogue Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku6_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN-3Dln4Exxm"
      },
      "source": [
        "There is actually a strong correlation between 'yesterday' promotion and 'today's' sales, almost making it impossible to not have a promotion the day before sales where up to >= 20K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pLcxHWhHGJ2"
      },
      "source": [
        "#### Store-End Lag Effects\n",
        "\n",
        "This will calculate and plot the amount of \"lag\" or \"impact\" 1 day of store-end promotion has over sales, e.g. promotion during 14 February has sales increase during 14, 15, 16 and 17 of February."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap2fscIHHGKF"
      },
      "source": [
        "##### SKU 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeAt48LGHGKG"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku1_total['date'], y=df_sku1_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku1_total.iterrows():\n",
        "    if row['Store End Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku1_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZqGV1WsHGKG"
      },
      "source": [
        "There is a mild correlation between '1-2 days' it most of the time have a promotion the day before sales where up to >= 40K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFP0uW6bHGKG"
      },
      "source": [
        "##### SKU 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyXe9ZUVHGKG"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku2_total['date'], y=df_sku2_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku2_total.iterrows():\n",
        "    if row['Store End Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku2_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlCW0bZDHGKH"
      },
      "source": [
        "There is actually a semi-strong correlation between 'yesterday' promotion and 'today's' sales, it does have 90% of the time a promotion the day before sales where up to >= 30K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNVeAiR9HGKH"
      },
      "source": [
        "##### SKU 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amZFiVSfHGKH"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku3_total['date'], y=df_sku3_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku3_total.iterrows():\n",
        "    if row['Store End Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku3_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yGP8aqdHGKH"
      },
      "source": [
        "There is actually a strong correlation between 'today's' promotion and 'today's' sales, almost making it impossible to not have a promotion the day before sales where up to >= 50K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOIz1DV5HGKI"
      },
      "source": [
        "##### SKU 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1dIi40_HGKI"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku4_total['date'], y=df_sku4_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku4_total.iterrows():\n",
        "    if row['Store End Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku4_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToSGjqpUHGKI"
      },
      "source": [
        "No correlation found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FRunoxUHGKI"
      },
      "source": [
        "##### SKU 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ufn6EoWbHGKJ"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku5_total['date'], y=df_sku5_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku5_total.iterrows():\n",
        "    if row['Store End Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku5_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liMHvSSVHGKJ"
      },
      "source": [
        "There is actually a strong correlation between 'yesterday' promotion and 'today's' sales, almost making it impossible to not have a promotion the day before sales where up to >= 20K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmvYTTTdHGKJ"
      },
      "source": [
        "##### SKU 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvRUysA4HGKK"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_sku6_total['date'], y=df_sku6_total['Sales'], mode='lines', name='Sales'))\n",
        "\n",
        "for index, row in df_sku6_total.iterrows():\n",
        "    if row['Store End Promo'] == 1:\n",
        "        fig.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=row['date'],\n",
        "            x1=row['date'],\n",
        "            y0=0,\n",
        "            y1=df_sku6_total['Sales'].max(),\n",
        "            line=dict(color=\"red\"),\n",
        "        )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    title=\"Sales Trend with Promotion Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGUCpzxlHGKK"
      },
      "source": [
        "There is a weak correlation between 'yesterday' promotion and 'today's' sales, it sometimes have a promotion the day before sales where up to >= 20K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwA2hRDuoC-p"
      },
      "source": [
        "## Sales by Date and Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7Y41yPNmPRA"
      },
      "outputs": [],
      "source": [
        "unique_products = df['Product'].unique()\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "for product in unique_products:\n",
        "    product_data = df[df['Product'] == product]\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=product_data['date'],\n",
        "        y=product_data['Sales'],\n",
        "        mode='lines+markers',\n",
        "        name=product\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Sales Over Time by Product\",\n",
        "    xaxis_title=\"Date\",\n",
        "    yaxis_title=\"Sales\",\n",
        "    xaxis=dict(type='date')\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6OOhCUkce7c"
      },
      "source": [
        "Products don't seem to have any correlation between them in terms of sales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLmgwfK2rMXh"
      },
      "source": [
        "## Product Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6hpEUa9rN_R"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.bar(df, x='Product', color='Product', title='Product Sales')\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OkCq_ywcpi_"
      },
      "source": [
        "All products are equally sold by an amount of 200 rounded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWi0yO2Jr_oB"
      },
      "source": [
        "## Product Sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_pGrp1csBOv"
      },
      "outputs": [],
      "source": [
        "fig = px.bar(df, x='Product', y='Sales', color='Product', title='Product Sales')\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZOrgp_lc1a3"
      },
      "source": [
        "In order, the profit by product would be: SKU3, SKU1, SKU6, SKU4, SKU5 and SKU2, as Pie Plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gn33ASVeH2B"
      },
      "outputs": [],
      "source": [
        "labels = df['Product'].unique()\n",
        "sku_sums = {}\n",
        "\n",
        "for sku in labels:\n",
        "    sku_sum = df[df['Product'] == sku]['Sales'].sum()\n",
        "    sku_sums[sku] = sku_sum\n",
        "\n",
        "sizes = [sku_sums[sku] for sku in labels]\n",
        "\n",
        "colors = ['blue', 'red', 'green', 'purple', 'orange', 'cyan']\n",
        "\n",
        "plt.pie(sizes, labels=labels, colors=colors,\n",
        "autopct='%1.1f%%', shadow=True, startangle=140)\n",
        "\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqAcJQ5tgee9"
      },
      "source": [
        "## Probability Distribution | Sales over Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ILJ7zivgik3"
      },
      "outputs": [],
      "source": [
        "from plotly.colors import n_colors\n",
        "\n",
        "grouped_df = df.groupby('Product')['Sales'].apply(list).reset_index()\n",
        "\n",
        "colors = n_colors('rgb(255, 40, 40)', 'rgb(255, 255, 10)', 12, colortype='rgb')\n",
        "\n",
        "fig = go.Figure()\n",
        "for data_line, day_name, color in zip(grouped_df['Sales'], grouped_df['Product'], colors):\n",
        "    fig.add_trace(go.Violin(x=data_line, line_color=color, name=day_name))\n",
        "\n",
        "fig.update_traces(orientation='h', side='positive', width=3, points=False)\n",
        "fig.update_layout(xaxis_showgrid=False, xaxis_zeroline=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTRGnB9Xil3R"
      },
      "source": [
        "### Confidence Intervals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Coe92EHtilLn"
      },
      "outputs": [],
      "source": [
        "def bootstrap_mean(data, n_bootstrap=1000, alpha=0.05):\n",
        "    bootstrap_means = []\n",
        "    n_samples = len(data)\n",
        "\n",
        "    for _ in range(n_bootstrap):\n",
        "        bootstrap_sample = np.random.choice(data, size=n_samples, replace=True)\n",
        "        bootstrap_mean = np.mean(bootstrap_sample)\n",
        "        bootstrap_means.append(bootstrap_mean)\n",
        "\n",
        "    lower_ci = np.percentile(bootstrap_means, 100 * alpha / 2)\n",
        "    upper_ci = np.percentile(bootstrap_means, 100 * (1 - alpha / 2))\n",
        "    return lower_ci, upper_ci"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu_nE9ObitWB"
      },
      "outputs": [],
      "source": [
        "sku1_ci_lower, sku1_ci_upper = bootstrap_mean(df_sku1_total['Sales'])\n",
        "sku2_ci_lower, sku2_ci_upper = bootstrap_mean(df_sku2_total['Sales'])\n",
        "sku3_ci_lower, sku3_ci_upper = bootstrap_mean(df_sku3_total['Sales'])\n",
        "sku4_ci_lower, sku4_ci_upper = bootstrap_mean(df_sku4_total['Sales'])\n",
        "sku5_ci_lower, sku5_ci_upper = bootstrap_mean(df_sku5_total['Sales'])\n",
        "sku6_ci_lower, sku6_ci_upper = bootstrap_mean(df_sku6_total['Sales'])\n",
        "\n",
        "print(\"Confidence Intervals for Sales over Product:\")\n",
        "print(f\"SKU1: [{sku1_ci_lower:.2f}, {sku1_ci_upper:.2f}]\")\n",
        "print(f\"SKU2: [{sku2_ci_lower:.2f}, {sku2_ci_upper:.2f}]\")\n",
        "print(f\"SKU3: [{sku3_ci_lower:.2f}, {sku3_ci_upper:.2f}]\")\n",
        "print(f\"SKU4: [{sku4_ci_lower:.2f}, {sku4_ci_upper:.2f}]\")\n",
        "print(f\"SKU5: [{sku5_ci_lower:.2f}, {sku5_ci_upper:.2f}]\")\n",
        "print(f\"SKU6: [{sku6_ci_lower:.2f}, {sku6_ci_upper:.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXaHKS5xae8i"
      },
      "source": [
        "## Probability Distribution | Discount over Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzMdKZkmae8j"
      },
      "outputs": [],
      "source": [
        "from plotly.colors import n_colors\n",
        "\n",
        "grouped_df = df.groupby('Product')['Price Discount (%)'].apply(list).reset_index()\n",
        "\n",
        "for i in range(0,5):\n",
        "  grouped_df.iloc[i,1].sort()\n",
        "\n",
        "colors = n_colors('rgb(20, 150, 40)', 'rgb(20, 255, 255)', 12, colortype='rgb')\n",
        "\n",
        "fig = go.Figure()\n",
        "for data_line, day_name, color in zip(grouped_df['Price Discount (%)'], grouped_df['Product'], colors):\n",
        "    fig.add_trace(go.Violin(x=data_line, line_color=color, name=day_name))\n",
        "\n",
        "fig.update_traces(orientation='h', side='positive', width=3, points=False)\n",
        "fig.update_layout(xaxis_showgrid=False, xaxis_zeroline=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilg-QHtWae8j"
      },
      "source": [
        "### Confidence Intervals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc7m5K7Xae8j"
      },
      "outputs": [],
      "source": [
        "sku1_ci_lower, sku1_ci_upper = bootstrap_mean(df_sku1_total['Price Discount (%)'])\n",
        "sku2_ci_lower, sku2_ci_upper = bootstrap_mean(df_sku2_total['Price Discount (%)'])\n",
        "sku3_ci_lower, sku3_ci_upper = bootstrap_mean(df_sku3_total['Price Discount (%)'])\n",
        "sku4_ci_lower, sku4_ci_upper = bootstrap_mean(df_sku4_total['Price Discount (%)'])\n",
        "sku5_ci_lower, sku5_ci_upper = bootstrap_mean(df_sku5_total['Price Discount (%)'])\n",
        "sku6_ci_lower, sku6_ci_upper = bootstrap_mean(df_sku6_total['Price Discount (%)'])\n",
        "\n",
        "print(\"Confidence Intervals for Price Discount (%) over Product:\")\n",
        "print(f\"SKU1: [{sku1_ci_lower:.2f}, {sku1_ci_upper:.2f}]\")\n",
        "print(f\"SKU2: [{sku2_ci_lower:.2f}, {sku2_ci_upper:.2f}]\")\n",
        "print(f\"SKU3: [{sku3_ci_lower:.2f}, {sku3_ci_upper:.2f}]\")\n",
        "print(f\"SKU4: [{sku4_ci_lower:.2f}, {sku4_ci_upper:.2f}]\")\n",
        "print(f\"SKU5: [{sku5_ci_lower:.2f}, {sku5_ci_upper:.2f}]\")\n",
        "print(f\"SKU6: [{sku6_ci_lower:.2f}, {sku6_ci_upper:.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0blyRKdDgDtZ"
      },
      "source": [
        "## Correlation Heatmap | Holidays\n",
        "\n",
        "A heatmap over the mean effect holidays have over the change ratio of the sales of each product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKRM7AjNgGZ_"
      },
      "outputs": [],
      "source": [
        "min = df_sorted_date['Change_Ratio'].min()\n",
        "max = df_sorted_date['Change_Ratio'].max()\n",
        "\n",
        "df_sorted_date['Change_Ratio_Norm'] = (df_sorted_date['Change_Ratio'] - min) / (max - min) * 2 - 1\n",
        "\n",
        "pivot_df = df_sorted_date.pivot_table(index='Product', columns=['V_DAY', 'EASTER', 'Covid_Flag', 'CHRISTMAS'], values='Change_Ratio_Norm', aggfunc='mean')\n",
        "x_binary_columns = ['V_DAY', 'EASTER', 'Covid_Flag', 'CHRISTMAS']\n",
        "pivot_df = pivot_df.dropna(axis=1)\n",
        "\n",
        "y_product_categories = pivot_df.index.tolist()\n",
        "\n",
        "heatmap = go.Figure(data=go.Heatmap(\n",
        "    z=pivot_df.values,\n",
        "    x=x_binary_columns,\n",
        "    y=y_product_categories,\n",
        "    colorscale='Viridis'))\n",
        "\n",
        "heatmap.update_layout(\n",
        "    title='Sales Heatmap',\n",
        "    xaxis_title='Binary Columns',\n",
        "    yaxis_title='Product')\n",
        "\n",
        "heatmap.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpO0SsuhpZgF"
      },
      "source": [
        "**Negative strong correlations (<= -15%):**\n",
        "* SKU2 - V_DAY\n",
        "* SKU4 - EASTER\n",
        "* SKU5 - EASTER\n",
        "\n",
        "**Positive strong correlations (>= 10%):**\n",
        "* SKU1 - EASTER, CHRISTMAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z5uulo_lfpl"
      },
      "source": [
        "## Change Ratio over Holidays\n",
        "\n",
        "Supports on a Time Series plot the conclusions made with Heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMerLs-ulk2J"
      },
      "source": [
        "### Easter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1mCNDbYiXUw"
      },
      "outputs": [],
      "source": [
        "products = df_sorted_date['Product'].unique()\n",
        "\n",
        "colors = ['blue', 'green', 'black', 'purple', 'orange', 'pink']\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "for product, color in zip(products, colors):\n",
        "    product_data = df_sorted_date[df_sorted_date['Product'] == product]\n",
        "    fig.add_trace(go.Scatter(x=product_data['date'], y=product_data['Change_Ratio_Norm'], mode='lines', name=product, line=dict(color=color)))\n",
        "\n",
        "    for index, row in product_data.iterrows():\n",
        "        if row['EASTER'] == 1:\n",
        "            fig.add_shape(\n",
        "                type=\"rect\",\n",
        "                x0=row['date'],\n",
        "                x1=row['date'],\n",
        "                y0=df_sorted_date['Change_Ratio_Norm'].min(),\n",
        "                y1=df_sorted_date['Change_Ratio_Norm'].max(),\n",
        "                line=dict(color=\"red\"),\n",
        "            )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Change_Ratio_Norm\",\n",
        "    title=\"Change_Ratio_Norm Trend with Holidays Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rwqELZ4ntxQ"
      },
      "source": [
        "### Chirstmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UjwML97ntxY"
      },
      "outputs": [],
      "source": [
        "products = df_sorted_date['Product'].unique()\n",
        "\n",
        "colors = ['blue', 'green', 'black', 'purple', 'orange', 'pink']\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "for product, color in zip(products, colors):\n",
        "    product_data = df_sorted_date[df_sorted_date['Product'] == product]\n",
        "    fig.add_trace(go.Scatter(x=product_data['date'], y=product_data['Change_Ratio_Norm'], mode='lines', name=product, line=dict(color=color)))\n",
        "\n",
        "    for index, row in product_data.iterrows():\n",
        "        if row['CHRISTMAS'] == 1:\n",
        "            fig.add_shape(\n",
        "                type=\"rect\",\n",
        "                x0=row['date'],\n",
        "                x1=row['date'],\n",
        "                y0=df_sorted_date['Change_Ratio_Norm'].min(),\n",
        "                y1=df_sorted_date['Change_Ratio_Norm'].max(),\n",
        "                line=dict(color=\"red\"),\n",
        "            )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Change_Ratio_Norm\",\n",
        "    title=\"Change_Ratio_Norm Trend with Holidays Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "571wF9Z0oK56"
      },
      "source": [
        "### Valentine's day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJBNk1_coK6C"
      },
      "outputs": [],
      "source": [
        "products = df_sorted_date['Product'].unique()\n",
        "\n",
        "colors = ['blue', 'green', 'black', 'purple', 'orange', 'pink']\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "for product, color in zip(products, colors):\n",
        "    product_data = df_sorted_date[df_sorted_date['Product'] == product]\n",
        "    fig.add_trace(go.Scatter(x=product_data['date'], y=product_data['Change_Ratio_Norm'], mode='lines', name=product, line=dict(color=color)))\n",
        "\n",
        "    for index, row in product_data.iterrows():\n",
        "        if row['V_DAY'] == 1:\n",
        "            fig.add_shape(\n",
        "                type=\"rect\",\n",
        "                x0=row['date'],\n",
        "                x1=row['date'],\n",
        "                y0=df_sorted_date['Change_Ratio_Norm'].min(),\n",
        "                y1=df_sorted_date['Change_Ratio_Norm'].max(),\n",
        "                line=dict(color=\"red\"),\n",
        "            )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"date\",\n",
        "    yaxis_title=\"Change_Ratio_Norm\",\n",
        "    title=\"Change_Ratio_Norm Trend with Holidays Periods\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imZ68BT-PWC_"
      },
      "source": [
        "## STL Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApWJqpSRPYls"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtUWWJ_oPal4"
      },
      "outputs": [],
      "source": [
        "time_series = df_sorted_date['Sales']\n",
        "decomposition = seasonal_decompose(time_series, model='additive', period=12)\n",
        "\n",
        "decomposed_df = pd.DataFrame({\n",
        "    'Date': df_sorted_date['date'],\n",
        "    'Trend': decomposition.trend,\n",
        "    'Seasonal': decomposition.seasonal,\n",
        "    'Residual': decomposition.resid\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV7VDdBaPbzt"
      },
      "outputs": [],
      "source": [
        "fig = px.line(decomposed_df, x='Date', y=['Trend', 'Seasonal', 'Residual'],\n",
        "              title='Time Series Decomposition')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3mycrzyURa0"
      },
      "source": [
        "By looking at the unstable trend and noisy seasonal component of the plot, this concludes that 'Sales' alone does not have a predictable pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsCyJrU1bHaS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV2hb2QRYY7w"
      },
      "source": [
        "## SelectKBest\n",
        "\n",
        "Here, I calculate the top 1 to 5 features for optimal sales rate for each product using sklearn SelectKBest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1TyTlbKbGie"
      },
      "source": [
        "### SKU1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ij6IpJ8Yao4"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "X = df_sku1_total.drop(\"Sales\", axis=1)\n",
        "X = X.drop(\"Product\", axis=1)\n",
        "X = X.drop(\"date\", axis=1)\n",
        "X = X.dropna()\n",
        "\n",
        "y = df_sku1_total[\"Sales\"]\n",
        "y = y.dropna()\n",
        "\n",
        "for i in range(1,6):\n",
        "  k_best_selector = SelectKBest(score_func=f_regression, k=i)\n",
        "\n",
        "  X_new = k_best_selector.fit_transform(X, y)\n",
        "\n",
        "  selected_indices = k_best_selector.get_support(indices=True)\n",
        "\n",
        "  selected_features = X.columns[selected_indices]\n",
        "\n",
        "  print(f\"Best {i} features for SKU1: \",selected_features.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLkC6sFEbIkw"
      },
      "source": [
        "### SKU2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYQFgKEfbIkw"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "X = df_sku2_total.drop(\"Sales\", axis=1)\n",
        "X = X.drop(\"Product\", axis=1)\n",
        "X = X.drop(\"date\", axis=1)\n",
        "X = X.dropna()\n",
        "\n",
        "y = df_sku2_total[\"Sales\"]\n",
        "y = y.dropna()\n",
        "\n",
        "for i in range(1,6):\n",
        "  k_best_selector = SelectKBest(score_func=f_regression, k=i)\n",
        "\n",
        "  X_new = k_best_selector.fit_transform(X, y)\n",
        "\n",
        "  selected_indices = k_best_selector.get_support(indices=True)\n",
        "\n",
        "  selected_features = X.columns[selected_indices]\n",
        "\n",
        "  print(f\"Best {i} features for SKU2: \",selected_features.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFaZ5jATbTp4"
      },
      "source": [
        "### SKU3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOOs2PelbTqA"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "X = df_sku3_total.drop(\"Sales\", axis=1)\n",
        "X = X.drop(\"Product\", axis=1)\n",
        "X = X.drop(\"date\", axis=1)\n",
        "X = X.dropna()\n",
        "\n",
        "y = df_sku3_total[\"Sales\"]\n",
        "y = y.dropna()\n",
        "\n",
        "for i in range(1,6):\n",
        "  k_best_selector = SelectKBest(score_func=f_regression, k=i)\n",
        "\n",
        "  X_new = k_best_selector.fit_transform(X, y)\n",
        "\n",
        "  selected_indices = k_best_selector.get_support(indices=True)\n",
        "\n",
        "  selected_features = X.columns[selected_indices]\n",
        "\n",
        "  print(f\"Best {i} features for SKU3: \",selected_features.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVp3ZNqDcJaK"
      },
      "source": [
        "### SKU4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3G7fyghcJaK"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "X = df_sku4_total.drop(\"Sales\", axis=1)\n",
        "X = X.drop(\"Product\", axis=1)\n",
        "X = X.drop(\"date\", axis=1)\n",
        "X = X.dropna()\n",
        "\n",
        "y = df_sku4_total[\"Sales\"]\n",
        "y = y.dropna()\n",
        "\n",
        "for i in range(1,6):\n",
        "  k_best_selector = SelectKBest(score_func=f_regression, k=i)\n",
        "\n",
        "  X_new = k_best_selector.fit_transform(X, y)\n",
        "\n",
        "  selected_indices = k_best_selector.get_support(indices=True)\n",
        "\n",
        "  selected_features = X.columns[selected_indices]\n",
        "\n",
        "  print(f\"Best {i} features for SKU4: \",selected_features.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T9rWoGScMhg"
      },
      "source": [
        "### SKU5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbFNBAPrcMhg"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "X = df_sku5_total.drop(\"Sales\", axis=1)\n",
        "X = X.drop(\"Product\", axis=1)\n",
        "X = X.drop(\"date\", axis=1)\n",
        "X = X.dropna()\n",
        "\n",
        "y = df_sku5_total[\"Sales\"]\n",
        "y = y.dropna()\n",
        "\n",
        "for i in range(1,6):\n",
        "  k_best_selector = SelectKBest(score_func=f_regression, k=i)\n",
        "\n",
        "  X_new = k_best_selector.fit_transform(X, y)\n",
        "\n",
        "  selected_indices = k_best_selector.get_support(indices=True)\n",
        "\n",
        "  selected_features = X.columns[selected_indices]\n",
        "\n",
        "  print(f\"Best {i} features for SKU5: \",selected_features.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kD6ClM4cUHY"
      },
      "source": [
        "### SKU6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FKOliaccUHk"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "X = df_sku6_total.drop(\"Sales\", axis=1)\n",
        "X = X.drop(\"Product\", axis=1)\n",
        "X = X.drop(\"date\", axis=1)\n",
        "X = X.dropna()\n",
        "\n",
        "y = df_sku6_total[\"Sales\"]\n",
        "y = y.dropna()\n",
        "\n",
        "for i in range(1,6):\n",
        "  k_best_selector = SelectKBest(score_func=f_regression, k=i)\n",
        "\n",
        "  X_new = k_best_selector.fit_transform(X, y)\n",
        "\n",
        "  selected_indices = k_best_selector.get_support(indices=True)\n",
        "\n",
        "  selected_features = X.columns[selected_indices]\n",
        "\n",
        "  print(f\"Best {i} features for SKU6: \",selected_features.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwSmRpBQc3Jp"
      },
      "source": [
        "## RandomForestRegressor\n",
        "\n",
        "Here using RandomForestRegressor from sklearn, I calculate the % of importance each feature has over total sales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGtFVSb5qqrx"
      },
      "source": [
        "### Global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeszhdH0dB-4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "X = df_sorted_date.drop(\"Sales\", axis=1)\n",
        "X = X.drop(\"date\", axis=1)\n",
        "X = X.drop(\"Change_Ratio\", axis=1)\n",
        "X = X.drop(\"Change_Ratio_Norm\", axis=1)\n",
        "X = X.drop(\"Product\", axis=1)\n",
        "X = X.dropna()\n",
        "y = df_sorted_date[\"Sales\"]\n",
        "y = y.dropna()\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "feature_importances = feature_importances * 100\n",
        "sorted_feature_indices = feature_importances.argsort()[::-1]\n",
        "sorted_features = X.columns[sorted_feature_indices]\n",
        "\n",
        "for feature, importance in zip(sorted_features, feature_importances[sorted_feature_indices]):\n",
        "    print(f\"{feature}: {round(importance,2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHEyZsSkqt1v"
      },
      "source": [
        "### SKU1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-zBiSRNqt13"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "X = df_sku1_total.drop(\"Sales\", axis=1)\n",
        "X = X.drop(\"date\", axis=1)\n",
        "X = X.drop(\"Product\", axis=1)\n",
        "X = X.dropna()\n",
        "y = df_sku1_total[\"Sales\"]\n",
        "y = y.dropna()\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "feature_importances = feature_importances * 100\n",
        "sorted_feature_indices = feature_importances.argsort()[::-1]\n",
        "sorted_features = X.columns[sorted_feature_indices]\n",
        "\n",
        "for feature, importance in zip(sorted_features, feature_importances[sorted_feature_indices]):\n",
        "    print(f\"{feature}: {round(importance,2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QjYyrvQq3PX"
      },
      "source": [
        "### SKU2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGkOBDUMq3PX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "X = df_sku2_total.drop(\"Sales\", axis=1)\n",
        "X = X.drop(\"date\", axis=1)\n",
        "X = X.drop(\"Product\", axis=1)\n",
        "X = X.dropna()\n",
        "y = df_sku2_total[\"Sales\"]\n",
        "y = y.dropna()\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "feature_importances = feature_importances * 100\n",
        "sorted_feature_indices = feature_importances.argsort()[::-1]\n",
        "sorted_features = X.columns[sorted_feature_indices]\n",
        "\n",
        "for feature, importance in zip(sorted_features, feature_importances[sorted_feature_indices]):\n",
        "    print(f\"{feature}: {round(importance,2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0FcTrgSq32x"
      },
      "source": [
        "### SKU3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG33nZdIq32y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "X = df_sku3_total.drop(\"Sales\", axis=1)\n",
        "X = X.drop(\"date\", axis=1)\n",
        "X = X.drop(\"Product\", axis=1)\n",
        "X = X.dropna()\n",
        "y = df_sku3_total[\"Sales\"]\n",
        "y = y.dropna()\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "feature_importances = feature_importances * 100\n",
        "sorted_feature_indices = feature_importances.argsort()[::-1]\n",
        "sorted_features = X.columns[sorted_feature_indices]\n",
        "\n",
        "for feature, importance in zip(sorted_features, feature_importances[sorted_feature_indices]):\n",
        "    print(f\"{feature}: {round(importance,2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDT-R8dGq4Uv"
      },
      "source": [
        "### SKU4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejZsKQhvq4Uv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "X = df_sku4_total.drop(\"Sales\", axis=1)\n",
        "X = X.drop(\"date\", axis=1)\n",
        "X = X.drop(\"Product\", axis=1)\n",
        "X = X.dropna()\n",
        "y = df_sku4_total[\"Sales\"]\n",
        "y = y.dropna()\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "feature_importances = feature_importances * 100\n",
        "sorted_feature_indices = feature_importances.argsort()[::-1]\n",
        "sorted_features = X.columns[sorted_feature_indices]\n",
        "\n",
        "for feature, importance in zip(sorted_features, feature_importances[sorted_feature_indices]):\n",
        "    print(f\"{feature}: {round(importance,2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqX4A8ypq4yP"
      },
      "source": [
        "### SKU5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KU4DbJOaq4yP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "X = df_sku5_total.drop(\"Sales\", axis=1)\n",
        "X = X.drop(\"date\", axis=1)\n",
        "X = X.drop(\"Product\", axis=1)\n",
        "X = X.dropna()\n",
        "y = df_sku5_total[\"Sales\"]\n",
        "y = y.dropna()\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "feature_importances = feature_importances * 100\n",
        "sorted_feature_indices = feature_importances.argsort()[::-1]\n",
        "sorted_features = X.columns[sorted_feature_indices]\n",
        "\n",
        "for feature, importance in zip(sorted_features, feature_importances[sorted_feature_indices]):\n",
        "    print(f\"{feature}: {round(importance,2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VZ_VGN5q5S3"
      },
      "source": [
        "### SKU6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN8HA9Ryq5S3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "X = df_sku6_total.drop(\"Sales\", axis=1)\n",
        "X = X.drop(\"date\", axis=1)\n",
        "X = X.drop(\"Product\", axis=1)\n",
        "X = X.dropna()\n",
        "y = df_sku6_total[\"Sales\"]\n",
        "y = y.dropna()\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "feature_importances = feature_importances * 100\n",
        "sorted_feature_indices = feature_importances.argsort()[::-1]\n",
        "sorted_features = X.columns[sorted_feature_indices]\n",
        "\n",
        "for feature, importance in zip(sorted_features, feature_importances[sorted_feature_indices]):\n",
        "    print(f\"{feature}: {round(importance,2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaxQ1z-PjGUt"
      },
      "source": [
        "## Gradient Boosting Regression\n",
        "\n",
        "Same as with Random Forest, but using a different algorithm to compare and beef up variable impact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBTW7q1ZjUt9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "feature_importances = feature_importances * 100\n",
        "sorted_feature_indices = feature_importances.argsort()[::-1]\n",
        "sorted_features = X.columns[sorted_feature_indices]\n",
        "\n",
        "for feature, importance in zip(sorted_features, feature_importances[sorted_feature_indices]):\n",
        "    print(f\"{feature}: {round(importance,2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mikVmOuPjscD"
      },
      "source": [
        "## PCA (Principal Component Analysis)\n",
        "\n",
        "Similar as before, but instead of calculating % of impact for each variable, it reduces the dataset dimensionality (samples) capturing the maximun variance (orthogonal rows), it could be useful for later training or beef ups validations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N9jblBIj-QD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_PCA = X\n",
        "X_PCA['Sales'] = y\n",
        "X_scaled = scaler.fit_transform(X_PCA)\n",
        "\n",
        "pca = PCA()\n",
        "\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "loadings = pca.components_\n",
        "\n",
        "loadings_df = pd.DataFrame(loadings, columns=X.columns)\n",
        "\n",
        "loadings_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDqRTPObpcS8"
      },
      "source": [
        "## Recursive Feature Elimination\n",
        "\n",
        "This also applies Random Forest but with 'feature_selection' so it returns the k more relevant features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLLN4YD-pdW9"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "X = df_sorted_date.drop('Sales', axis=1)\n",
        "X = X.drop('date', axis=1)\n",
        "X = X.drop('Product', axis=1)\n",
        "X = X.drop('Change_Ratio', axis=1)\n",
        "X = X.dropna()\n",
        "y = df_sorted_date[\"Sales\"]\n",
        "y = y.dropna()\n",
        "y = y.drop(y.index[-1])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "regressor = RandomForestRegressor()\n",
        "\n",
        "for i in range(3,6):\n",
        "  rfe = RFE(estimator=regressor, n_features_to_select=i)\n",
        "\n",
        "  rfe.fit(X_train, y_train)\n",
        "\n",
        "  selected_features = X.columns[rfe.support_]\n",
        "  print(f\"Selected {i} features:\", selected_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH9iBGAqKm1t"
      },
      "source": [
        "## Recommendations based on EDA\n",
        "\n",
        "1. Price Discount is the most impactful feature within the dataset to optimize sales for every product, whenever sales are dropping low, it would be the best to offer discounts.\n",
        "\n",
        "2. SKU1, 3 and 6 have the majority of the sales, investments should prioritize these 3 products.\n",
        "\n",
        "3. During Christmas and Easter, SKU1 is the optimal product for sales, it would have > 10% of it's standard sales rate during that day.\n",
        "\n",
        "4. For products from SKU1 to SKU5, offering In-Store promotions would increase sales the same day or the day after the promotion.\n",
        "\n",
        "5. For all products, offering Catalogue promotions would increase sales the day after the promotion.\n",
        "\n",
        "6. For SKU4, there is a huge increase on sales the day after a Store-End promotion was chosen.\n",
        "\n",
        "7. Each product has a 95% probability of generating these amount of sales daily (to prepare budget before hand):\n",
        "\n",
        "* SKU1: [43574.16, 52112.24]\n",
        "* SKU2: [6107.43, 8654.14]\n",
        "* SKU3: [49403.63, 63337.02]\n",
        "* SKU4: [14931.67, 19247.07]\n",
        "* SKU5: [14460.09, 18752.23]\n",
        "* SKU6: [32916.42, 43159.34]\n",
        "\n",
        "8. Google Mobility has no effect over the sales of any product, so there should be no need to close off sales given this situation.\n",
        "\n",
        "9. The following discounts ([0,1] <=> [0%, 100%]) are optimal for product sales increase and company's profit balance:\n",
        "\n",
        "* SKU1: [0.11, 0.15]\n",
        "* SKU2: [0.14, 0.19]\n",
        "* SKU3: [0.42, 0.50]\n",
        "* SKU4: [0.39, 0.46]\n",
        "* SKU5: [0.22, 0.28]\n",
        "* SKU6: [0.36, 0.42]\n",
        "\n",
        "> NOTE: These recommendations are only based on EDA which needs double checking with model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhbrNYIdDUE3"
      },
      "source": [
        "# Model Proposals TTT - Training | Tuning | Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aejavNiIDob5"
      },
      "source": [
        "## Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-oW_iiEDqJ6"
      },
      "outputs": [],
      "source": [
        "df_sorted_date = df_sorted_date.dropna()\n",
        "df_sorted_date.drop('Change_Ratio', axis=1, inplace=True)\n",
        "df_sorted_date.drop('year', axis=1, inplace=True)\n",
        "df_sorted_date = pd.get_dummies(df_sorted_date, columns=['Product'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuTxnFokEqy6"
      },
      "outputs": [],
      "source": [
        "df_sorted_date = df_sorted_date[df_sorted_date['Sales'] != 0]\n",
        "df_sorted_date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RARPXKh8Dgha"
      },
      "source": [
        "## ARIMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSBWohyUHKMl"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "!pip install pmdarima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLkHNFLcDhes"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import TimestampType, FloatType\n",
        "from pmdarima.arima import auto_arima\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ARIMA Prediction\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df_spark = spark.createDataFrame(df_sorted_date)\n",
        "\n",
        "df_spark = df_spark.withColumn(\"date\", col(\"date\").cast(TimestampType()))\n",
        "selected_features = [feature for feature in df_spark.columns if feature != 'date']\n",
        "\n",
        "features_df = df_spark.select(selected_features)\n",
        "\n",
        "features_pandas = features_df.toPandas()\n",
        "\n",
        "arima_model = auto_arima(features_pandas['Sales'], seasonal=False, suppress_warnings=True)\n",
        "\n",
        "predictions = arima_model.predict(n_periods=len(features_pandas))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFg34kmBLKkV"
      },
      "outputs": [],
      "source": [
        "predictions = predictions.reset_index(drop=True)\n",
        "features_pandas['Sales_Predicted'] = predictions\n",
        "\n",
        "features_df = spark.createDataFrame(features_pandas)\n",
        "\n",
        "features_df.show(100)\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSZsnbCzQkdB"
      },
      "source": [
        "## Results\n",
        "\n",
        "AIRMA can't handle complex non-linear relationships between variables, nor binary columns, nor multivariable training, leading to a mean prediction which doesn't supply the expected output for the company, so I'll be moving to the next option for forecast non-multivariable model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO8u4CxnRCz8"
      },
      "source": [
        "## Prophet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90Y-6Oh3RZOg"
      },
      "outputs": [],
      "source": [
        "!pip install prophet\n",
        "!pip install pystan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9ls1XEdREPd"
      },
      "outputs": [],
      "source": [
        "from prophet import Prophet\n",
        "\n",
        "prophet_df = df_sorted_date[['date', 'Sales']]\n",
        "prophet_df.columns = ['ds', 'y']\n",
        "\n",
        "model = Prophet()\n",
        "model.fit(prophet_df)\n",
        "\n",
        "future = model.make_future_dataframe(periods=1)\n",
        "\n",
        "forecast = model.predict(future)\n",
        "\n",
        "predicted_sales = forecast[['ds', 'yhat']]\n",
        "\n",
        "merged_df = pd.merge(prophet_df, predicted_sales, on='ds', how='inner')\n",
        "\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfetAdTrZ--X"
      },
      "source": [
        "## Results\n",
        "\n",
        "Prophet fails because it's a model for date only predictions, and by results from EDA, this dataset doesn't have sales patterns alone for it to predict, so moving to the next model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBiRYp_WbawC"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp0iOqA9bb_T"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(layers.LSTM(64, input_shape=(1, 16), activation='relu', return_sequences=True))\n",
        "model.add(layers.LSTM(64, activation='relu'))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "X = df_sorted_date.drop(columns=['Sales', 'date', 'Google_Mobility', 'Covid_Flag'])\n",
        "y = df_sorted_date['Sales']\n",
        "\n",
        "sequence_length = 1\n",
        "num_features = X.shape[1]\n",
        "\n",
        "X_sequences = []\n",
        "y_sequences = []\n",
        "\n",
        "for i in range(len(X) - sequence_length + 1):\n",
        "    X_sequences.append(X.iloc[i:i+sequence_length].values)\n",
        "    y_sequences.append(y.iloc[i+sequence_length-1])\n",
        "\n",
        "X_sequences = np.array(X_sequences)\n",
        "y_sequences = np.array(y_sequences)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_sequences, y_sequences, test_size=0.3, random_state=42)\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train, y_train, epochs=1500, batch_size=64, validation_data=(X_val, y_val))\n",
        "\n",
        "predictions_list = []\n",
        "\n",
        "for sequence in X_val:\n",
        "    prediction = model.predict(np.expand_dims(sequence, axis=0))\n",
        "    predictions_list.append(prediction[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uyd8UH2_hBLW"
      },
      "outputs": [],
      "source": [
        "absolute_errors = np.abs(predictions_list - y_val)\n",
        "\n",
        "sum_absolute_errors = np.sum(absolute_errors)\n",
        "sum_actual_values = np.sum(y_val)\n",
        "\n",
        "error_measure = 1 - (sum_absolute_errors / sum_actual_values)\n",
        "\n",
        "print(\"Class Forecast Accuracy:\", error_measure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQWwtHHujQJY"
      },
      "outputs": [],
      "source": [
        "percentage_differences = []\n",
        "\n",
        "for i in range(101):\n",
        "    sequence = X_val[i]\n",
        "\n",
        "    prediction = model.predict(np.expand_dims(sequence, axis=0))\n",
        "\n",
        "    actual_value = y_val[i]\n",
        "    percentage_diff = ((prediction - actual_value) / actual_value) * 100\n",
        "    percentage_differences.append(percentage_diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQqJsZZPmZO8"
      },
      "outputs": [],
      "source": [
        "mean_percentage_diff = np.mean(percentage_differences)\n",
        "\n",
        "print(\"Mean Percentage Difference:\", mean_percentage_diff)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "LSTM is by this point the best model for predictions based on all feature columns provided, it handles perfectly complex correlations and multivariable dependecies, making it's Class Forecast Accuracy being 76% (tested with 100 samples, a mean difference of 11% predictions vs actual values), given that this model has been the only one that actually could learn and make reasonable predictions, I'll use it as the final result of the project if the next models doesn't overcome these results."
      ],
      "metadata": {
        "id": "iCUiZ5olCxVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gated Recurrent Unit\n",
        "\n",
        "Originally this was going to be XGBoost, however, given that it could only return the % relevance of each feature for optimal sales (which is already covered in Feature Engineering section), then this new model will replace it."
      ],
      "metadata": {
        "id": "rs5ZypI0G6lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "\n",
        "X = df_sorted_date.drop(columns=['Sales', 'date', 'Google_Mobility', 'Covid_Flag'])\n",
        "y = df_sorted_date['Sales']\n",
        "\n",
        "sequence_length = 1\n",
        "\n",
        "X_sequences = []\n",
        "y_sequences = []\n",
        "\n",
        "for i in range(len(X) - sequence_length + 1):\n",
        "    X_sequences.append(X.iloc[i:i+sequence_length].values)\n",
        "    y_sequences.append(y.iloc[i+sequence_length-1])\n",
        "\n",
        "X_sequences = np.array(X_sequences)\n",
        "y_sequences = np.array(y_sequences)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_sequences, test_size=0.3, random_state=42)\n",
        "\n",
        "model = Sequential([\n",
        "    GRU(64, input_shape=(1, X_train.shape[-1]), activation='relu', return_sequences=True),\n",
        "    GRU(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "predictions = model.predict(X_test)"
      ],
      "metadata": {
        "id": "BdOcKwWYHhtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percentage_differences = []\n",
        "\n",
        "for i in range(101):\n",
        "    sequence = X_val[i]\n",
        "\n",
        "    prediction = model.predict(np.expand_dims(sequence, axis=0))\n",
        "\n",
        "    actual_value = y_val[i]\n",
        "    percentage_diff = ((prediction - actual_value) / actual_value) * 100\n",
        "    percentage_differences.append(percentage_diff)"
      ],
      "metadata": {
        "id": "TXgncfhFUSBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_percentage_diff = np.mean(percentage_differences)\n",
        "\n",
        "print(\"Mean Percentage Difference:\", mean_percentage_diff)"
      ],
      "metadata": {
        "id": "TxXI2LcDUUxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "Even if it was capable of multivariable training, it's performance wasn't better than LSTM, so it's discarded."
      ],
      "metadata": {
        "id": "FgjqxtcLVu5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-Beats\n",
        "\n",
        "Since N-beats supports 1 variable training (forecast seasonality and trend), same results as prophet are expected so this option is discarded before-hand."
      ],
      "metadata": {
        "id": "zIvnFh0EWHLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting Regression"
      ],
      "metadata": {
        "id": "R3taeKo9cBJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X = df_sorted_date.drop(['Sales', 'date'], axis=1)\n",
        "y = df_sorted_date['Sales']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1, random_state=42)\n",
        "\n",
        "gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "errors = []\n",
        "for i in range(0, 101):\n",
        "  y_pred = gb_regressor.predict(X_test.iloc[[i]])\n",
        "  errors.append(abs(100 - ((y_pred[0] * 100) / y_test.iloc[i])))\n"
      ],
      "metadata": {
        "id": "fjWlodY7cClK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_error = sum(errors) / len(errors)\n",
        "print(mean_error)"
      ],
      "metadata": {
        "id": "vMHhewxIexlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "This is the second best model so far with margin error of 18%."
      ],
      "metadata": {
        "id": "dVQnYidqgHbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Results"
      ],
      "metadata": {
        "id": "N3FeHNrMY_Bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM was the best model for predictions from the 5 model list proposed in week 11 + Gradient Boosting Regression, it has a best case accuracy of 76% with 11% error margin per prediction, it's input data in order of relevance goes as (based on EDA and prediction testing):\n",
        "\n",
        "1. Price Discount (%) (Contributes at least 40%)\n",
        "4. ProductX (The product to sale) (Contributes at least 25%)\n",
        "2. Day of month (Contributes at least 15%)\n",
        "3. Month of year (Contributes at least 10%)\n",
        "5. In-Store Promo (Contributes at least 5%)\n",
        "6. Store End Promo (Contributes at least 5%)\n",
        "\n",
        "and undefined/lowest relevance input features are the rest of the original features provided in the dataset along with a daily change ratio (this can be calculated with value 0 starting at day 0, and by applying difference substraction daily from there)\n",
        "\n",
        "However, holiday and promotions effects are also described in EDA results section, which are imporant and should not be discarded based only on the model's weights and biases.\n",
        "\n",
        "So, any new day the company could use this model to predict for example how much payback would product SKU1 produce at the end of the day 15 of October of any year, and use EDA results to add promotions and holiday awareness to the model and so get an approximate of the expected profit that day.\n",
        "\n",
        "Forecasting in weekly buckets would not only be possible by creating 7 predictions in order, but also it would be customizable as each parameter could be different from each day of the forecasted week, and for the best, it wouldn't be misguided by past week's unlikely events."
      ],
      "metadata": {
        "id": "3Z9cL618ZA9-"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}